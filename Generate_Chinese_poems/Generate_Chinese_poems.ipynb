{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week11_NLP_codelab_Seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "D1r5gV29LLWJ"
      },
      "cell_type": "markdown",
      "source": [
        " ",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GwjBYH6hgPRP"
      },
      "cell_type": "markdown",
      "source": [
        "# Build a sequence to sequence language model to generate Chinese poems\n",
        "We train a seq2seq RNN on ~43,000 poems from the Tang Dynastry to learn the probability distribution of the next word/character in the sequence given the history of previous characters."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dnXFHqP7fmJw"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess the file containing all the poems\n",
        "\n",
        "The format of our input data is like this:\n",
        "\n",
        "`(optional title + \":\")poem`\n",
        "\n",
        "We will do:\n",
        "\n",
        "- Remove title\n",
        "- Remove spaces\n",
        "- Remove empty symbols\n",
        "- Replace other symbols"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HDxfxaNbE1Na",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "data_filename ='poetry.txt'\n",
        "poems = []\n",
        "with open(data_filename, \"r\") as in_file:\n",
        "  for line in in_file.readlines():\n",
        "    line = line.strip()\n",
        "    # find title if exists\n",
        "    if ':' in line:\n",
        "      line = line.split(':')\n",
        "    # some poems are empty\n",
        "    if len(line) == 2:\n",
        "      poem = line[1]\n",
        "    else:\n",
        "      continue\n",
        "    # discard if contains special symbols\n",
        "    if re.search(r'[(（《_□]', poem):\n",
        "      continue\n",
        "    # discard if too short or too long\n",
        "    if len(poem) < 5 or len(poem) > 40:\n",
        "      continue\n",
        "    # remove symbols\n",
        "    poem = re.sub(u'[，。]','',poem)\n",
        "    poems.append(poem)\n",
        "\n",
        "poems = np.random.permutation(poems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFp7XEY-m1xg",
        "colab_type": "code",
        "outputId": "0cb29e95-8d51-481e-f52a-7755595a5bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "poems[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'闭门茅底偶为邻北阮那怜南阮贫却是梅花无世态隔墙分送一枝春'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "LgKt-uNJm2KU",
        "colab_type": "code",
        "outputId": "72a9d07f-e7e4-4897-d72f-6a5db0ac3d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "poems[2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'杜若溪边手自移旋抽烟剑碧参差何时织得孤帆去悬向秋风访所思'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mFjPmxWXlZJ2"
      },
      "cell_type": "markdown",
      "source": [
        "We select 5 poems as our test set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tqbNdpZYO8ma",
        "outputId": "44e9d27a-f0b8-48b6-90b5-b6227ff73e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "poems_train, poems_test = poems[:-5], poems[-5:]\n",
        "len(poems_train), len(poems_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11098, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sb9em1KyfuvZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a vocabulary mapping word to id using Tokenizer!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I1zXYlrxPMPx",
        "outputId": "804087ef-dcae-411c-fbd2-69e75c2852be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "poem_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True)\n",
        "# Create word to ID dictionary\n",
        "poem_tokenizer.fit_on_texts(poems)\n",
        "# Get dictionary\n",
        "word_index = poem_tokenizer.word_index\n",
        "\n",
        "# Note that ID starts from 1!!\n",
        "# We need to add special ID 0\n",
        "word_index[\"<PAD>\"] = 0\n",
        "# Create ID to word \n",
        "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
        "print(\"Number of unique chars: {}\".format(len(word_index)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique chars: 4762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SeIp4Nnsmtbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "29iNZFMwKzB-"
      },
      "cell_type": "markdown",
      "source": [
        "Again, always check if there is any strange symbols in the dictionary. Here we only print first and last parts."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8Bqy83LtQWjA",
        "outputId": "f54b3eca-6b68-4421-e7c9-81b7747fd434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# sort word index by ID\n",
        "for (w,i) in sorted(word_index.items(), key=lambda w: w[1]):\n",
        "# print some words to check if there are errors!\n",
        "  if i > 10 and i < len(word_index)-5: continue\n",
        "  print(\"{} {}\".format(w,i))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PAD> 0\n",
            "不 1\n",
            "人 2\n",
            "一 3\n",
            "山 4\n",
            "风 5\n",
            "无 6\n",
            "花 7\n",
            "来 8\n",
            "日 9\n",
            "春 10\n",
            "屩 4757\n",
            "羖 4758\n",
            "锱 4759\n",
            "殖 4760\n",
            "豗 4761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BPVYU_l3qUq_",
        "outputId": "b139e529-9e1a-4c7a-b0db-394328311d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# Apply word to ID on training and test set\n",
        "poems_train = poem_tokenizer.texts_to_sequences(poems_train)\n",
        "poems_test = poem_tokenizer.texts_to_sequences(poems_test)\n",
        "# Check and see if there is any error\n",
        "print(poems_train[0])\n",
        "print(''.join([reverse_word_index[w] for w in poems_train[0]]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[577, 55, 791, 643, 708, 32, 640, 180, 1587, 424, 256, 54, 1587, 720, 130, 22, 504, 7, 6, 218, 1214, 402, 644, 172, 188, 3, 119, 10]\n",
            "闭门茅底偶为邻北阮那怜南阮贫却是梅花无世态隔墙分送一枝春\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nPBqvnAKf0Me"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data for input\n",
        "\n",
        "We flatten the input to a long list."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "reJzVQlkLgs_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# flatten to a long string of characters\n",
        "poems_train = [w for po in poems_train for w in po]\n",
        "\n",
        "# flatten to a long string of characters\n",
        "poems_test = [w for po in poems_test for w in po]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rj9yNonAf49F"
      },
      "cell_type": "markdown",
      "source": [
        "## Define an input object\n",
        "\n",
        "We need to put the input into batches.\n",
        "* Reshape input data into a rectangular matrix and crop remainders\n",
        "* Calculate shape of each batch\n",
        "* Generate batch with input and output = input shift by one time step\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YXoDJKqbTYnz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mNVdwFpgmoeq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PoemInput(object):\n",
        "  def __init__(self, data, config, name=None):\n",
        "    self.batch_size = batch_size = config.batch_size\n",
        "    self.num_steps = num_steps = config.num_steps\n",
        "    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
        "    self.sources, self.targets = self.input_producer(\n",
        "        data, batch_size, num_steps, name=name)\n",
        "\n",
        "  def input_producer(self, raw_data, batch_size, num_steps, name=None):\n",
        "    \"\"\"Reshape the poem data to form input and output.\n",
        "    This chunks the raw_data into batches of examples and returns Tensors that\n",
        "    are drawn from these batches.\n",
        "    Args:\n",
        "      raw_data: a list of words\n",
        "      batch_size: int, the batch size.\n",
        "      num_steps: int, the sequence length.\n",
        "      name: the name of this operation (optional).\n",
        "    Returns:\n",
        "      A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
        "      of the tuple is the same data time-shifted to the right by one.\n",
        "    \"\"\"\n",
        "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
        "    # get size of the 1-d tensor\n",
        "    data_len = tf.size(raw_data)\n",
        "    # calculate how many batches\n",
        "    batch_len = data_len // batch_size\n",
        "    # crop data that does not fit in a batch\n",
        "    data = tf.reshape(raw_data[0:batch_size*batch_len],\n",
        "                      [batch_size, batch_len])\n",
        "    # calculate how many batches in an epoch\n",
        "    epoch_size = (batch_len-1) // num_steps\n",
        "    # make sure there is at least one batch\n",
        "    assertion = tf.assert_positive(epoch_size,\n",
        "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "    with tf.control_dependencies([assertion]):\n",
        "      epoch_size = tf.cast(tf.identity(epoch_size, name=\"epoch_size\"), tf.int64)\n",
        "\n",
        "    # start generating slices\n",
        "    # range_input_producer returns a sequence of IDs \n",
        "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
        "    x = data[:, i*num_steps  :(i+1)*num_steps]\n",
        "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6k4i6WJ8gFLl"
      },
      "cell_type": "markdown",
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QhBnCcTTUpLS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "class Hparam(object):\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 5\n",
        "  num_layers = 1\n",
        "  num_steps = 35\n",
        "  vocab_size = len(word_index)\n",
        "  embedding_size = 100\n",
        "  hidden_size = 100\n",
        "  warmup_epochs = 3\n",
        "  num_epochs_to_train = 20\n",
        "  keep_prob = 0.6\n",
        "  lr_decay = 0.9\n",
        "  batch_size = 100\n",
        "\n",
        "config = Hparam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WyOWLTIVgJWZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Construct model\n",
        "In this step, the entire model structure must be defined completely. Including\n",
        "* Input\n",
        "* Size of layers\n",
        "* Connection between layers\n",
        "* Variables in layers\n",
        "* Output\n",
        "* Loss\n",
        "* Operations that apply the gradients (optimizer)\n",
        "* Placeholder for feeding special values\n",
        "* Properties that can be read from outside\n",
        "\n",
        "Note that we will use CudnnLSTM to speed up our training if available. However, I will provide two versions of LSTM here in case you cannot find a machine with GPUs."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5tuOJamJS6gm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
        "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
        "from tensorflow.nn import embedding_lookup, dropout\n",
        "\n",
        "# Build our model\n",
        "class MySeq2SeqModel(object):\n",
        "  def __init__(self, is_training, config, input_):\n",
        "    self._is_training = is_training\n",
        "    self._input = input_\n",
        "    self._cell = None\n",
        "    self.batch_size = input_.batch_size\n",
        "    self.num_steps = input_.num_steps\n",
        "    rnn_size = config.hidden_size\n",
        "    vocab_size = config.vocab_size\n",
        "    embedding_size = config.embedding_size\n",
        "\n",
        "    # Embeddings can only exist on CPU\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "      embedding_weights = tf.get_variable(\"embedding\", \\\n",
        "                     [vocab_size, embedding_size])\n",
        "      embed_inputs = tf.nn.embedding_lookup(embedding_weights, input_.sources)\n",
        "\n",
        "    if is_training and config.keep_prob < 1.:\n",
        "      embed_inputs = tf.nn.dropout(embed_inputs, config.keep_prob)\n",
        "\n",
        "    # build RNN using CudnnLSTM\n",
        "    output, _ = self._build_rnn(embed_inputs, config, is_training)\n",
        "    # build RNN using basic LSTM\n",
        "    # output, _ = self._build_rnn_old_lstm(embed_inputs, config, is_training)\n",
        "\n",
        "    # Remember RNN output is [batch_size x time, rnnsize]\n",
        "    # Dense layer for projecting onto vocabulary size\n",
        "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
        "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
        "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
        "    # Reshape logits to be a 3-D tensor for sequence loss\n",
        "    logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
        "    self._logits = logits\n",
        "\n",
        "    # Use the contrib sequence loss and average over the batches\n",
        "    loss = tf.contrib.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        input_.targets,\n",
        "        tf.ones([self.batch_size, self.num_steps]),\n",
        "        average_across_timesteps=False,\n",
        "        average_across_batch=True)\n",
        "\n",
        "    # Update the cost\n",
        "    self._cost = tf.reduce_sum(loss)\n",
        "\n",
        "    if not is_training:\n",
        "      return\n",
        "\n",
        "    # A variable to store learning rate\n",
        "    self._lr = tf.Variable(0.0, trainable=False)\n",
        "\n",
        "    # Calculate gradients\n",
        "    # Get a list of trainable variables\n",
        "    tvars = tf.trainable_variables()\n",
        "    # Get gradient and clip by norm\n",
        "    grads, _ = tf.clip_by_global_norm(\\\n",
        "                 tf.gradients(self._cost, tvars),\n",
        "                 config.max_grad_norm)\n",
        "    # Define an optimizer\n",
        "    # Note that the optimizer reads the value of learning rate from variable\n",
        "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
        "    # Define an operation that actually applies the gradients\n",
        "    self._train_op = optimizer.apply_gradients(\n",
        "        zip(grads, tvars),\n",
        "        global_step=tf.train.get_or_create_global_step())\n",
        "    # A placeholder for feeding new learning rates\n",
        "    self._new_lr = tf.placeholder(\n",
        "         tf.float32, shape=[], name=\"new_learning_rate\")\n",
        "    self._lr_update_op = tf.assign(self._lr, self._new_lr)\n",
        "  \n",
        "  def _build_rnn(self, inputs, config, is_training):\n",
        "    # RNN requires time-major\n",
        "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
        "    self._cell = CudnnLSTM(\n",
        "        num_layers=config.num_layers,\n",
        "        num_units=config.hidden_size,\n",
        "        )\n",
        "    self._cell.build(inputs.get_shape())\n",
        "    outputs, state = self._cell(inputs)\n",
        "    # Transpose from time-major to batch-major\n",
        "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
        "    # Reshape from [batch, time, rnnsize] to [batch x time, rnnsize]\n",
        "    # For computing softmax later\n",
        "    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
        "    return outputs, state\n",
        "\n",
        "  def _build_rnn_old_lstm(self, inputs, config, is_training):\n",
        "    def make_cell():\n",
        "      cell = BasicLSTMCell(\n",
        "        config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
        "        reuse=not is_training)\n",
        "      if is_training and config.keep_prob < 1:\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(\n",
        "            cell, output_keep_prob=config.keep_prob)\n",
        "      return cell\n",
        "\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(\n",
        "        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
        "\n",
        "    self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
        "    state = self._initial_state\n",
        "    outputs = []\n",
        "    inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
        "    outputs, state = tf.nn.static_rnn(cell, inputs,\n",
        "                                      initial_state=self._initial_state)\n",
        "    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
        "    return output, state\n",
        "  \n",
        "  def assign_lr(self, session, lr_value):\n",
        "    session.run(self._lr_update_op, feed_dict={self._new_lr: lr_value})\n",
        "\n",
        "  @property\n",
        "  def input(self):\n",
        "    return self._input\n",
        "\n",
        "  @property\n",
        "  def cost(self):\n",
        "    return self._cost\n",
        "\n",
        "  @property\n",
        "  def lr(self):\n",
        "    return self._lr\n",
        "\n",
        "  @property\n",
        "  def train_op(self):\n",
        "    return self._train_op\n",
        "\n",
        "  @property\n",
        "  def logits(self):\n",
        "    return self._logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7EYipAzwgXMD"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a training operation for an epoch\n",
        "This procedure gets the output from the model for each batch.\n",
        "We need a dictionary with these keys:\n",
        "\n",
        "* \"cost\": Reads the propertie `model.cost` that we defined above. \n",
        "* \"do_op\": Perform operation `model.train_op` that applies gradients\n",
        "\n",
        "After running (calling `session.run()`), the same key will contain the return values.\n",
        "\n",
        "We can add any key in the dictionary that corresponds to `@property` in the model!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "84z2L2sggp48",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(session, model, do_op=None, verbose=False):\n",
        "  start_time = time.time()\n",
        "  costs = 0.0\n",
        "  iters = 0\n",
        "  feed_to_model_dict = {\n",
        "      \"cost\": model.cost,\n",
        "  }\n",
        "  # if an operation is provided, put that in the feed\n",
        "  if do_op is not None:\n",
        "    feed_to_model_dict[\"do_op\"] = do_op\n",
        "\n",
        "  for step in range(model.input.epoch_size):\n",
        "    # use the session to run, feed the dictionary\n",
        "    s_out = session.run(feed_to_model_dict)\n",
        "    # The returned dictionary will contain the information we need\n",
        "    cost = s_out[\"cost\"]\n",
        "    # Accumulate cost\n",
        "    costs += cost\n",
        "    # Accumulate number of training steps\n",
        "    iters += model.input.num_steps\n",
        "    # Print loss periodically\n",
        "    if verbose and (step+1) % (model.input.epoch_size // 5) == 0:\n",
        "      print(\"%.0f%% ppl: %.3f, speed: %.0f char/sec\" %\n",
        "            ((step+1) * 100.0 / model.input.epoch_size, \\\n",
        "             np.exp(costs/iters), \\\n",
        "             iters * model.input.batch_size/(time.time() - start_time)))\n",
        "\n",
        "  return np.exp(costs / iters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NHRlVdNelCkx"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a Generator\n",
        "We will also create a Generator Model to generate new poems. Note that it is much less complicated than the training model.\n",
        "However, we need to add a procedure to generate output for some steps. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sZ0VsXVuqBM1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyGeneratorModel(object):\n",
        "  def __init__(self, config):\n",
        "    self._input = tf.placeholder(tf.int32, shape=[1], name=\"_input\")\n",
        "    self.batch_size = 1\n",
        "    self.num_steps = config.num_steps\n",
        "    rnn_size = config.hidden_size\n",
        "    vocab_size = config.vocab_size\n",
        "    embedding_size = config.embedding_size\n",
        "\n",
        "    # Embeddings can only exist on CPU\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "      embedding_weights = tf.get_variable(\"embedding\", \\\n",
        "                     [vocab_size, embedding_size])\n",
        "      embed_inputs = tf.nn.embedding_lookup(embedding_weights, self._input)\n",
        "      embed_inputs = tf.expand_dims(embed_inputs, 0)\n",
        "\n",
        "    # build RNN using CudnnLSTM\n",
        "    self._cell = CudnnLSTM(\n",
        "        num_layers=config.num_layers,\n",
        "        num_units=config.hidden_size,\n",
        "        )\n",
        "\n",
        "    # build final projection layer\n",
        "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
        "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
        "\n",
        "    # Collect a sequence of output word IDs\n",
        "    self._output_word_ids = []\n",
        "\n",
        "    # Decode first word\n",
        "    outputs, state = self._cell(embed_inputs)\n",
        "    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
        "    logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
        "    # Get input for next step\n",
        "    next_input = tf.argmax(logits, axis=-1)\n",
        "    next_input = tf.squeeze(next_input)\n",
        "    self._output_word_ids.append(next_input)\n",
        "    # Convert next input to word embeddings\n",
        "    next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
        "    next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
        "    \n",
        "    # Feed back to LSTM\n",
        "    for _ in range(self.num_steps-1):\n",
        "      outputs, state = self._cell(next_input, state)\n",
        "      outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
        "      logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
        "      next_input = tf.argmax(logits, axis=-1)\n",
        "      next_input = tf.squeeze(next_input)\n",
        "      self._output_word_ids.append(next_input)\n",
        "\n",
        "      next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
        "      next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
        "\n",
        "  @property\n",
        "  def output_word_ids(self):\n",
        "    return self._output_word_ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-oOtiwCr0lrD"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a call to generator\n",
        "Again we need a decoder to translate word IDs back to words. And we need to define a procedure to communicate with the model. `feed_dict` and `fetches` are two keys to do that."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CB9TLfWilDg9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_text(text, max_len_newline=5):\n",
        "  words = [reverse_word_index.get(i, \"<UNK>\") for i in text]\n",
        "  fixed_width_string = []\n",
        "\n",
        "  for w_pos in range(len(words)):\n",
        "    fixed_width_string.append(words[w_pos])\n",
        "    if (w_pos+1) % max_len_newline == 0:\n",
        "      fixed_width_string.append('\\n')\n",
        "  return ''.join(fixed_width_string)\n",
        "\n",
        "def run_generator(session, model, seed_word, config):\n",
        "  \n",
        "  feed_to_model_dict = {\n",
        "      model._input: [seed_word],\n",
        "  }\n",
        "  fetch_model_dict = {\n",
        "      \"output_word_ids\": model.output_word_ids\n",
        "  }\n",
        "\n",
        "  # An example of sending and receiving data from the model\n",
        "  vals = session.run(fetches=fetch_model_dict, feed_dict=feed_to_model_dict)\n",
        "  output_word_ids = vals['output_word_ids']\n",
        "\n",
        "  # Decode to readable words\n",
        "  print(decode_text([seed_word] + output_word_ids, (config.num_steps+1)//4))\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Qvm0wW2inUwk"
      },
      "cell_type": "markdown",
      "source": [
        "## Main training controller\n",
        "Finally, we define a controller that:\n",
        "* Create the model for training\n",
        "* Create the model for testing, copying from the training model\n",
        "* Prepare the input data\n",
        "* Define what to log in the progress of training\n",
        "* Create a `session` that communicates with computation graph\n",
        "* Change learning rate optionally\n",
        "* Get test set results\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hKsN1qiVhdms",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(_):\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
        "\n",
        "    with tf.name_scope(\"Train\"):\n",
        "      # Create input producer\n",
        "      train_input = PoemInput(poems_train, config, name=\"TrainInput\")\n",
        "      # Create the model instance\n",
        "      with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
        "        m = MySeq2SeqModel(is_training=True, config=config, input_=train_input)\n",
        "      # Add information to logs\n",
        "      tf.summary.scalar(\"Training_Loss\", m.cost)\n",
        "      tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
        "\n",
        "    with tf.name_scope(\"Test\"):\n",
        "      eval_config = Hparam()\n",
        "      eval_config.batch_size = 1\n",
        "      eval_config.num_steps = 20\n",
        "\n",
        "      # Create another input for test data\n",
        "      # Note that eval_config was set locally\n",
        "      test_input = PoemInput(poems_test, eval_config, name=\"TestInput\")\n",
        "      # Create another model but reuse the variables in the training model\n",
        "      with tf.variable_scope(\"Model\", reuse=True):\n",
        "        mtest = MySeq2SeqModel(is_training=False, config=eval_config,\n",
        "                         input_=test_input)\n",
        "\n",
        "    with tf.name_scope(\"Gen\"):\n",
        "      generator_config = Hparam()\n",
        "      generator_config.batch_size = 1\n",
        "      generator_config.num_steps = 19\n",
        "      # Create generator model\n",
        "      with tf.variable_scope(\"Model\", reuse=True):\n",
        "        mgenerate = MyGeneratorModel(config=generator_config)\n",
        "\n",
        "    # Hardware settings\n",
        "    config_proto = tf.ConfigProto(allow_soft_placement=True)\n",
        "    # Create a MonitoredTrainingSession that controls the training process\n",
        "    # Also automatically logs and reports \n",
        "    # Note the `checkpoint_dir` setting\n",
        "    with tf.train.MonitoredTrainingSession(checkpoint_dir=\"logs\", \\\n",
        "                                           config=config_proto, \\\n",
        "                                           log_step_count_steps=-1) as session:\n",
        "      for i in range(config.num_epochs_to_train):\n",
        "\n",
        "        # Calculate learning rate decay\n",
        "        lr_decay = config.lr_decay ** max(i + 1 - config.warmup_epochs, 0.0)\n",
        "        # Set learning rate\n",
        "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
        "        # Print new learning rate\n",
        "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
        "        # Train one epoch and report loss\n",
        "        train_perplexity = run_epoch(session, m, do_op=m.train_op, verbose=True)\n",
        "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
        "      \n",
        "      # End of training\n",
        "      # Evaluate test set performance\n",
        "      test_perplexity = run_epoch(session, mtest)\n",
        "      print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
        "      \n",
        "      # Set a seed word and generate new poem\n",
        "      seed_word = '天'\n",
        "      run_generator(session, mgenerate, seed_word=word_index[seed_word], config=generator_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g_6KfSfUnbC5"
      },
      "cell_type": "markdown",
      "source": [
        "## Start training\n",
        "We can actually start training by calling the controller."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "y3f2BoLtl9Yo",
        "outputId": "7a05ea04-c8df-498f-a240-6c06d776686b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2927
        }
      },
      "cell_type": "code",
      "source": [
        "main(1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-025c1f343f94>:40: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into logs/model.ckpt.\n",
            "Epoch: 1 Learning rate: 1.000\n",
            "20% ppl: 2379.114, speed: 88689 char/sec\n",
            "40% ppl: 1819.984, speed: 107282 char/sec\n",
            "59% ppl: 1619.322, speed: 117008 char/sec\n",
            "79% ppl: 1500.410, speed: 122714 char/sec\n",
            "99% ppl: 1423.376, speed: 126474 char/sec\n",
            "Epoch: 1 Train Perplexity: 1420.294\n",
            "Epoch: 2 Learning rate: 1.000\n",
            "20% ppl: 1159.594, speed: 143921 char/sec\n",
            "40% ppl: 1137.561, speed: 124196 char/sec\n",
            "59% ppl: 1128.941, speed: 129980 char/sec\n",
            "79% ppl: 1116.562, speed: 133277 char/sec\n",
            "99% ppl: 1110.106, speed: 135350 char/sec\n",
            "Epoch: 2 Train Perplexity: 1110.136\n",
            "Epoch: 3 Learning rate: 1.000\n",
            "20% ppl: 1060.813, speed: 143808 char/sec\n",
            "40% ppl: 1044.578, speed: 144133 char/sec\n",
            "59% ppl: 1037.821, speed: 143946 char/sec\n",
            "79% ppl: 1019.026, speed: 143804 char/sec\n",
            "99% ppl: 1006.092, speed: 143697 char/sec\n",
            "Epoch: 3 Train Perplexity: 1005.717\n",
            "Epoch: 4 Learning rate: 0.900\n",
            "20% ppl: 918.604, speed: 143649 char/sec\n",
            "40% ppl: 902.118, speed: 143790 char/sec\n",
            "59% ppl: 898.437, speed: 143774 char/sec\n",
            "79% ppl: 885.444, speed: 143913 char/sec\n",
            "99% ppl: 876.731, speed: 143899 char/sec\n",
            "Epoch: 4 Train Perplexity: 876.205\n",
            "Epoch: 5 Learning rate: 0.810\n",
            "20% ppl: 809.242, speed: 141464 char/sec\n",
            "40% ppl: 796.870, speed: 142569 char/sec\n",
            "59% ppl: 796.412, speed: 142957 char/sec\n",
            "79% ppl: 786.939, speed: 143147 char/sec\n",
            "99% ppl: 777.733, speed: 143252 char/sec\n",
            "Epoch: 5 Train Perplexity: 777.171\n",
            "Epoch: 6 Learning rate: 0.729\n",
            "20% ppl: 714.770, speed: 141826 char/sec\n",
            "40% ppl: 701.333, speed: 142356 char/sec\n",
            "59% ppl: 699.998, speed: 143072 char/sec\n",
            "79% ppl: 691.018, speed: 143372 char/sec\n",
            "99% ppl: 683.102, speed: 143671 char/sec\n",
            "Epoch: 6 Train Perplexity: 682.672\n",
            "Epoch: 7 Learning rate: 0.656\n",
            "20% ppl: 633.762, speed: 141396 char/sec\n",
            "40% ppl: 626.752, speed: 142477 char/sec\n",
            "59% ppl: 626.102, speed: 143010 char/sec\n",
            "79% ppl: 618.797, speed: 142941 char/sec\n",
            "99% ppl: 612.585, speed: 143136 char/sec\n",
            "Epoch: 7 Train Perplexity: 612.401\n",
            "Epoch: 8 Learning rate: 0.590\n",
            "20% ppl: 575.803, speed: 143801 char/sec\n",
            "40% ppl: 571.531, speed: 143818 char/sec\n",
            "59% ppl: 572.599, speed: 143738 char/sec\n",
            "79% ppl: 567.332, speed: 143162 char/sec\n",
            "99% ppl: 561.964, speed: 143360 char/sec\n",
            "Epoch: 8 Train Perplexity: 561.811\n",
            "Epoch: 9 Learning rate: 0.531\n",
            "20% ppl: 536.071, speed: 142846 char/sec\n",
            "40% ppl: 532.158, speed: 143722 char/sec\n",
            "59% ppl: 532.087, speed: 143572 char/sec\n",
            "79% ppl: 528.613, speed: 142639 char/sec\n",
            "99% ppl: 523.894, speed: 142815 char/sec\n",
            "Epoch: 9 Train Perplexity: 523.794\n",
            "Epoch: 10 Learning rate: 0.478\n",
            "20% ppl: 503.017, speed: 143671 char/sec\n",
            "40% ppl: 500.254, speed: 143965 char/sec\n",
            "59% ppl: 500.875, speed: 144293 char/sec\n",
            "79% ppl: 496.933, speed: 144126 char/sec\n",
            "99% ppl: 492.854, speed: 144167 char/sec\n",
            "Epoch: 10 Train Perplexity: 492.738\n",
            "Epoch: 11 Learning rate: 0.430\n",
            "20% ppl: 476.217, speed: 145113 char/sec\n",
            "40% ppl: 475.149, speed: 142943 char/sec\n",
            "59% ppl: 475.026, speed: 143337 char/sec\n",
            "79% ppl: 472.164, speed: 143113 char/sec\n",
            "99% ppl: 468.360, speed: 143333 char/sec\n",
            "Epoch: 11 Train Perplexity: 468.301\n",
            "Epoch: 12 Learning rate: 0.387\n",
            "20% ppl: 456.275, speed: 143164 char/sec\n",
            "40% ppl: 454.382, speed: 143866 char/sec\n",
            "59% ppl: 455.066, speed: 143850 char/sec\n",
            "79% ppl: 451.911, speed: 143726 char/sec\n",
            "99% ppl: 447.998, speed: 143631 char/sec\n",
            "Epoch: 12 Train Perplexity: 447.876\n",
            "Epoch: 13 Learning rate: 0.349\n",
            "20% ppl: 438.935, speed: 143104 char/sec\n",
            "40% ppl: 436.623, speed: 142473 char/sec\n",
            "59% ppl: 437.314, speed: 141449 char/sec\n",
            "79% ppl: 434.270, speed: 141717 char/sec\n",
            "99% ppl: 430.905, speed: 141264 char/sec\n",
            "Epoch: 13 Train Perplexity: 430.758\n",
            "Epoch: 14 Learning rate: 0.314\n",
            "20% ppl: 423.862, speed: 144942 char/sec\n",
            "40% ppl: 422.331, speed: 143785 char/sec\n",
            "59% ppl: 422.534, speed: 143595 char/sec\n",
            "79% ppl: 420.005, speed: 143788 char/sec\n",
            "99% ppl: 416.782, speed: 143831 char/sec\n",
            "Epoch: 14 Train Perplexity: 416.630\n",
            "Epoch: 15 Learning rate: 0.282\n",
            "20% ppl: 409.466, speed: 144334 char/sec\n",
            "40% ppl: 409.571, speed: 143776 char/sec\n",
            "59% ppl: 409.760, speed: 143820 char/sec\n",
            "79% ppl: 407.719, speed: 144091 char/sec\n",
            "99% ppl: 404.569, speed: 142911 char/sec\n",
            "Epoch: 15 Train Perplexity: 404.433\n",
            "Epoch: 16 Learning rate: 0.254\n",
            "20% ppl: 399.714, speed: 144266 char/sec\n",
            "40% ppl: 399.011, speed: 143965 char/sec\n",
            "59% ppl: 398.876, speed: 143678 char/sec\n",
            "79% ppl: 396.647, speed: 143332 char/sec\n",
            "99% ppl: 393.617, speed: 143531 char/sec\n",
            "Epoch: 16 Train Perplexity: 393.496\n",
            "Epoch: 17 Learning rate: 0.229\n",
            "20% ppl: 389.904, speed: 144437 char/sec\n",
            "40% ppl: 388.866, speed: 144023 char/sec\n",
            "59% ppl: 388.701, speed: 143947 char/sec\n",
            "79% ppl: 386.832, speed: 143974 char/sec\n",
            "99% ppl: 383.672, speed: 143691 char/sec\n",
            "Epoch: 17 Train Perplexity: 383.618\n",
            "Epoch: 18 Learning rate: 0.206\n",
            "20% ppl: 381.784, speed: 143269 char/sec\n",
            "40% ppl: 380.782, speed: 143128 char/sec\n",
            "59% ppl: 380.571, speed: 143491 char/sec\n",
            "79% ppl: 378.514, speed: 143568 char/sec\n",
            "99% ppl: 375.684, speed: 143604 char/sec\n",
            "Epoch: 18 Train Perplexity: 375.634\n",
            "Epoch: 19 Learning rate: 0.185\n",
            "20% ppl: 374.969, speed: 142451 char/sec\n",
            "40% ppl: 373.586, speed: 143257 char/sec\n",
            "59% ppl: 373.467, speed: 143247 char/sec\n",
            "79% ppl: 371.571, speed: 143505 char/sec\n",
            "99% ppl: 368.725, speed: 143561 char/sec\n",
            "Epoch: 19 Train Perplexity: 368.633\n",
            "Epoch: 20 Learning rate: 0.167\n",
            "20% ppl: 368.063, speed: 144132 char/sec\n",
            "40% ppl: 367.560, speed: 143651 char/sec\n",
            "59% ppl: 367.503, speed: 143431 char/sec\n",
            "79% ppl: 365.666, speed: 143540 char/sec\n",
            "99% ppl: 362.755, speed: 143566 char/sec\n",
            "Epoch: 20 Train Perplexity: 362.730\n",
            "Test Perplexity: 714.256\n",
            "天下不知何\n",
            "处是春风不\n",
            "知何处去年\n",
            "年不得春风\n",
            "\n",
            "INFO:tensorflow:Saving checkpoints for 1620 into logs/model.ckpt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YgU5M-MXVB9x"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the training process shown here. Observe that training loss keeps decreasing, which means that the model is actually learning. \n",
        "\n",
        "Also, due to the speedup of CudnnLSTM, the speed can be very fast (> 100,000 w/s). Using basic LSTM can only achieve ~6,000 w/s.\n",
        "\n",
        "If you are running this script locally, start `tensorboard` and point it to the `logs` directory will allow you to see the loss plot over time. We will not be able to show that easily in Colab environment.\n",
        "\n",
        "You can also continue training by calling the controller again. Try this later and see if the poems generated gets better over time."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rW284a7xioaw"
      },
      "cell_type": "markdown",
      "source": [
        "## Clear previous output\n",
        "\n",
        "Tensorflow will automatically load previous models if you specify a path for the `session`. However, that will be a problem if you change some parts of the model. e.g., change embedding size, LSTM size, or number of layers.\n",
        "\n",
        "You will see something like \n",
        "```\n",
        "INFO:tensorflow:Restoring parameters from logs/model.ckpt-4465\n",
        "...\n",
        "InvalidArgumentError: Assign requires shapes of both tensors to match.\n",
        "```\n",
        "Always remember to clear output directory if you are experimenting with different model structures!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dfkNbkgyG81Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -R logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XWfT6DruhUBd"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "What we learned today:\n",
        "1. Preprocessing for language modeling data\n",
        "    * Create a dictionary that maps words to unique IDs\n",
        "    * Convert words to ID\n",
        "    * Reshape sequences to unified lengths\n",
        "    * Create a helper to produce data\n",
        "2. Building a model using tensorflow\n",
        "    * Hyperparameters\n",
        "    * Training operation\n",
        "    * Testing operation\n",
        "    * Control function\n",
        "3. Training and evaluation\n",
        "    * Observe loss\n",
        "    * Evaluate on test set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TDKza528g_pK"
      },
      "cell_type": "markdown",
      "source": [
        "# Appendix: connect your Google Drive to Colab for uploading your data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4Z3N08zCdhJG"
      },
      "cell_type": "markdown",
      "source": [
        "First, copy the file into Google Drive. Then run the following code to link your Drive to this notebook. Follow the link in the output."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NI3J6rPSqRHP",
        "outputId": "423ba499-4867-48c4-c22a-8918e0e0de45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MbRchpOzdgm2"
      },
      "cell_type": "markdown",
      "source": [
        "Copy (`cp`) the file from `/gdrive` to this server."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S-ObQ860GsUw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/poetry.txt ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EY_0LsbNK42a",
        "outputId": "3a37a975-10e4-40de-8228-781e2db76346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/Book*.txt ./"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/gdrive/My Drive/Colab Notebooks/Book*.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ue2W5_FFbZze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
